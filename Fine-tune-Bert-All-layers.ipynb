{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b134e19-4be7-42c7-ac2c-f41ca0a8e3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import accelerate\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import numpy as np\n",
    "import datasets\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2d89064-8bd4-4d3c-8620-04a79832b26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.25.1\n",
      "2.3.1+cpu\n",
      "4.41.2\n",
      "2.20.0\n",
      "0.31.0\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)\n",
    "print(datasets.__version__)\n",
    "print(accelerate.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7a0c90-5833-4e9b-90fa-53e848295d08",
   "metadata": {},
   "source": [
    "# BERT for Binary classification (Adding last softmax layer at the end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1882ac3b-ffe4-4d42-8fc0-fc79e61ca2b6",
   "metadata": {},
   "source": [
    "### Example 1: A few samples (update the whole weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fa5a7bd-ac67-42a2-a24e-56be671fd036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 01:35, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.694800</td>\n",
       "      <td>0.700712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.688800</td>\n",
       "      <td>0.699288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.658700</td>\n",
       "      <td>0.697982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.623400</td>\n",
       "      <td>0.691195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.641000</td>\n",
       "      <td>0.681630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.660900</td>\n",
       "      <td>0.663044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.616500</td>\n",
       "      <td>0.635565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.572200</td>\n",
       "      <td>0.596927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.547300</td>\n",
       "      <td>0.563259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.513000</td>\n",
       "      <td>0.521652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.6216712760925293, metrics={'train_runtime': 97.6657, 'train_samples_per_second': 2.56, 'train_steps_per_second': 0.512, 'total_flos': 1027777560000.0, 'train_loss': 0.6216712760925293, 'epoch': 50.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "# Sample data\n",
    "texts = [\"I love programming.\", \"I hate bugs.\", \"Machine learning is fascinating.\", \"Debugging is frustrating.\", \"I enjoy debugging.\"]\n",
    "labels = [1, 0, 1, 0, 1]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "texts_val = [\"I enjoy debugging.\", \"Programming can be tough.\"]\n",
    "val_encodings = tokenizer(texts_val, truncation=True, padding=True, max_length=128)\n",
    "labels_val = [1, 0]\n",
    "\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = CustomDataset(train_encodings, labels)\n",
    "val_dataset = CustomDataset(val_encodings, labels_val)  # Use the same data for validation for this example\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=5,\n",
    "    evaluation_strategy=\"steps\",  # This might still throw a warning, but you can try 'eval_strategy' instead\n",
    ")\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e065c1d-31cc-472e-9ba8-1931b3d96b67",
   "metadata": {},
   "source": [
    "#### evaluate the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18d0ccaf-b8e0-44c2-bb56-5cee6d502008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "texts_test = [\"I enjoy debugging.\", \"Programming can be tough.\", \"I hate debugging.\"]\n",
    "new_encodings = tokenizer(texts_test, truncation=True, padding=True, max_length=128)\n",
    "labels_test = [1, 0, 0]\n",
    "\n",
    "test_dataset = CustomDataset(new_encodings, labels_test)  # Use the same data for validation for this example\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Convert predictions to PyTorch tensor\n",
    "predictions_tensor = torch.tensor(predictions.predictions)\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_labels = torch.argmax(predictions_tensor, dim=1)\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1313c323-e326-4c53-801b-029f3894e264",
   "metadata": {},
   "source": [
    "### Example 2: 100 samples from IMDB (update the whole weights), each sample just 4 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e5b9fa6-a266-41d8-92c2-94498f70fbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "\n",
    "train_texts = dataset['train']['text']\n",
    "train_labels = dataset['train']['label']\n",
    "\n",
    "test_texts = dataset['test']['text']\n",
    "test_labels = dataset['test']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2cb229c-282e-472f-ab57-3806edada0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_labels[500:550]+train_labels[15000:15050]\n",
    "texts = train_texts[500:550]+ train_texts[15000:15050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40296e09-f567-43fc-9b1f-55d0faba6655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cdb83f69-eb68-47ea-9762-6d2843c1aea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 02:25, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.758000</td>\n",
       "      <td>0.704411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.685600</td>\n",
       "      <td>0.702425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.747900</td>\n",
       "      <td>0.699309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.715400</td>\n",
       "      <td>0.694544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.686800</td>\n",
       "      <td>0.688574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.688100</td>\n",
       "      <td>0.680474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>0.670621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.714500</td>\n",
       "      <td>0.657397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.671500</td>\n",
       "      <td>0.643004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.659600</td>\n",
       "      <td>0.629927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.653100</td>\n",
       "      <td>0.620213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.624200</td>\n",
       "      <td>0.602781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.620100</td>\n",
       "      <td>0.589123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=65, training_loss=0.6827584193303035, metrics={'train_runtime': 147.7578, 'train_samples_per_second': 3.384, 'train_steps_per_second': 0.44, 'total_flos': 1027777560000.0, 'train_loss': 0.6827584193303035, 'epoch': 5.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(texts, truncation=True, padding=True, max_length=4)\n",
    "train_dataset = CustomDataset(train_encodings, labels)\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=5,\n",
    "    evaluation_strategy=\"steps\",  # This might still throw a warning, but you can try 'eval_strategy' instead\n",
    ")\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48fed06-4812-46a5-8299-322d5a34480b",
   "metadata": {},
   "source": [
    "#### evaluate the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a97d367d-af93-4a9a-b535-140780093b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = test_labels[500:550]+test_labels[15000:15050]\n",
    "texts_test = test_texts[500:550]+ test_texts[15000:15050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5e9d8e5-da42-4274-be10-e43f8e050aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
      "        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
      "        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
      "        0, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "test_encodings = tokenizer(texts_test, truncation=True, padding=True, max_length=4)\n",
    "test_dataset = CustomDataset(test_encodings, labels_test)  # Use the same data for validation for this example\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Convert predictions to PyTorch tensor\n",
    "predictions_tensor = torch.tensor(predictions.predictions)\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_labels = torch.argmax(predictions_tensor, dim=1)\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8da2f7e4-ce49-47be-a9ae-510c52eeb7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20, 30],\n",
       "       [23, 27]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(labels_test, predicted_labels.numpy())\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d979371-0439-49e5-b2c9-9aa66ac6bbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.0 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(labels_test, predicted_labels.numpy())\n",
    "print(f'{accuracy*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d138576-05a0-45c7-8c99-04b0c1ea15d6",
   "metadata": {},
   "source": [
    "### Example 3: 100 samples from IMDB (update the whole weights), each sample just 16 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07115e8b-c5a0-4835-a09e-5f6069611178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\dghol\\Desktop\\new env\\new\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 04:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.753200</td>\n",
       "      <td>0.866059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.919000</td>\n",
       "      <td>0.853293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.856500</td>\n",
       "      <td>0.829978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.802827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.847800</td>\n",
       "      <td>0.766256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.781200</td>\n",
       "      <td>0.727027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.724600</td>\n",
       "      <td>0.691155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.711900</td>\n",
       "      <td>0.654524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.648300</td>\n",
       "      <td>0.628223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.685500</td>\n",
       "      <td>0.602586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.612100</td>\n",
       "      <td>0.571643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.609900</td>\n",
       "      <td>0.533288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.581600</td>\n",
       "      <td>0.483007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=65, training_loss=0.7346512574415941, metrics={'train_runtime': 276.6476, 'train_samples_per_second': 1.807, 'train_steps_per_second': 0.235, 'total_flos': 4111110240000.0, 'train_loss': 0.7346512574415941, 'epoch': 5.0})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(texts, truncation=True, padding=True, max_length=16)\n",
    "train_dataset = CustomDataset(train_encodings, labels)\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=5,\n",
    "    evaluation_strategy=\"steps\",  # This might still throw a warning, but you can try 'eval_strategy' instead\n",
    ")\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf7ac7f-643d-4d85-ac90-d74bf7e93da4",
   "metadata": {},
   "source": [
    "#### evaluate the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc069baa-293b-45eb-bee3-b61272dba597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
      "        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "test_encodings = tokenizer(texts_test, truncation=True, padding=True, max_length=16)\n",
    "test_dataset = CustomDataset(test_encodings, labels_test)  # Use the same data for validation for this example\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Convert predictions to PyTorch tensor\n",
    "predictions_tensor = torch.tensor(predictions.predictions)\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_labels = torch.argmax(predictions_tensor, dim=1)\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7764848-9e47-40bf-83de-f0201bd32ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24, 26],\n",
       "       [28, 22]], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(labels_test, predicted_labels.numpy())\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f02a6ebe-9a55-4e37-b375-adffa04a9dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.0 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(labels_test, predicted_labels.numpy())\n",
    "print(f'{accuracy*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce172e06-9788-4389-a551-391f069d4d49",
   "metadata": {},
   "source": [
    "### Example 4: 100 samples from IMDB (update the whole weights), each sample just 32 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d955087-b835-4d1b-a889-2da968bed555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\dghol\\Desktop\\new env\\new\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 07:21, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.671500</td>\n",
       "      <td>0.789865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.858400</td>\n",
       "      <td>0.781243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.735300</td>\n",
       "      <td>0.765248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.735600</td>\n",
       "      <td>0.743247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.811100</td>\n",
       "      <td>0.714885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.724100</td>\n",
       "      <td>0.683549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.672300</td>\n",
       "      <td>0.659176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.624200</td>\n",
       "      <td>0.639920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.667300</td>\n",
       "      <td>0.614588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.637300</td>\n",
       "      <td>0.594895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>0.569405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.549800</td>\n",
       "      <td>0.544222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.635700</td>\n",
       "      <td>0.511169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=65, training_loss=0.6906496598170354, metrics={'train_runtime': 445.457, 'train_samples_per_second': 1.122, 'train_steps_per_second': 0.146, 'total_flos': 8222220480000.0, 'train_loss': 0.6906496598170354, 'epoch': 5.0})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(texts, truncation=True, padding=True, max_length=32)\n",
    "train_dataset = CustomDataset(train_encodings, labels)\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=5,\n",
    "    evaluation_strategy=\"steps\",  # This might still throw a warning, but you can try 'eval_strategy' instead\n",
    ")\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175c5f13-2d68-4ee2-bdc9-cc5666910562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### evaluate the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5d3835e-bd1a-4d33-9e35-3f9117f9ccc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n",
      "        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
      "        0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "test_encodings = tokenizer(texts_test, truncation=True, padding=True, max_length=32)\n",
    "test_dataset = CustomDataset(test_encodings, labels_test)  # Use the same data for validation for this example\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Convert predictions to PyTorch tensor\n",
    "predictions_tensor = torch.tensor(predictions.predictions)\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_labels = torch.argmax(predictions_tensor, dim=1)\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a298be33-1b69-4482-b077-191636f4a64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32, 18],\n",
       "       [31, 19]], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(labels_test, predicted_labels.numpy())\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9f1195c-b5da-43bb-bb07-21efe28235da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.0 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(labels_test, predicted_labels.numpy())\n",
    "print(f'{accuracy*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12445d42-539b-41b5-b368-0cef91be688d",
   "metadata": {},
   "source": [
    "### Example 5: 300 samples from IMDB (update the whole weights), each sample just 64 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ee07b873-a9d7-463b-86ff-5ef0778899f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_labels[500:650]+train_labels[15000:15150]\n",
    "texts = train_texts[500:650]+ train_texts[15000:15150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5ec22d46-b1e0-44e0-b914-11873ee82652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "94dc3cc9-8f69-463d-9af6-359ed3d32633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\dghol\\Desktop\\new env\\new\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='760' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [760/760 3:09:09, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.744600</td>\n",
       "      <td>0.776092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.751100</td>\n",
       "      <td>0.741113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.689100</td>\n",
       "      <td>0.703499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.697600</td>\n",
       "      <td>0.667485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.667600</td>\n",
       "      <td>0.649557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.694500</td>\n",
       "      <td>0.627668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.664200</td>\n",
       "      <td>0.602618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.633600</td>\n",
       "      <td>0.576123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.614100</td>\n",
       "      <td>0.537760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.549400</td>\n",
       "      <td>0.476585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.534500</td>\n",
       "      <td>0.410813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.475900</td>\n",
       "      <td>0.348782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.443600</td>\n",
       "      <td>0.331611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.372600</td>\n",
       "      <td>0.227255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.379900</td>\n",
       "      <td>0.178938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.253000</td>\n",
       "      <td>0.153446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.159617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.135316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.274500</td>\n",
       "      <td>0.104591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.064400</td>\n",
       "      <td>0.053908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.078400</td>\n",
       "      <td>0.029171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.035775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.164300</td>\n",
       "      <td>0.010676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.050200</td>\n",
       "      <td>0.005906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.012572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.017789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>0.001670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.001717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.001097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.000910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.000543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=760, training_loss=0.13700935268425382, metrics={'train_runtime': 11355.7565, 'train_samples_per_second': 0.528, 'train_steps_per_second': 0.067, 'total_flos': 197333291520000.0, 'train_loss': 0.13700935268425382, 'epoch': 20.0})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(texts, truncation=True, padding=True, max_length=64)\n",
    "train_dataset = CustomDataset(train_encodings, labels)\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",  # This might still throw a warning, but you can try 'eval_strategy' instead\n",
    ")\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55c4ad5-f787-4047-899f-075ff8bde409",
   "metadata": {},
   "source": [
    "#### evaluate the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5bcb25b5-43f5-4e28-a7e1-eeedf1c425c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
      "        1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "test_encodings = tokenizer(texts_test, truncation=True, padding=True, max_length=64)\n",
    "test_dataset = CustomDataset(test_encodings, labels_test)  # Use the same data for validation for this example\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Convert predictions to PyTorch tensor\n",
    "predictions_tensor = torch.tensor(predictions.predictions)\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_labels = torch.argmax(predictions_tensor, dim=1)\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b1b89a08-c5cb-4ec3-9df9-8c3ba47dfc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34, 16],\n",
       "       [18, 32]], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(labels_test, predicted_labels.numpy())\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e3528e8a-566e-4bad-8f98-a8abd6ff0bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.0 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(labels_test, predicted_labels.numpy())\n",
    "print(f'{accuracy*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56502546-4ba9-4a33-9adf-46b1d574bf6d",
   "metadata": {},
   "source": [
    "### Example 6: 300 samples from IMDB (update the last layer weights), each sample just 64 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1bc99835-55cf-46a4-a53b-1e9ab71337b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_labels[500:650]+train_labels[15000:15150]\n",
    "texts = train_texts[500:650]+ train_texts[15000:15150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7823b6f4-30e0-4bab-8cea-6f2ae002b3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\dghol\\Desktop\\new env\\new\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='760' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [760/760 2:14:55, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.704600</td>\n",
       "      <td>0.703419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.712100</td>\n",
       "      <td>0.703203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.724800</td>\n",
       "      <td>0.702769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.668300</td>\n",
       "      <td>0.702563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.709100</td>\n",
       "      <td>0.702250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.701700</td>\n",
       "      <td>0.701815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.706900</td>\n",
       "      <td>0.701335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.685400</td>\n",
       "      <td>0.701067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.713000</td>\n",
       "      <td>0.700565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.701000</td>\n",
       "      <td>0.699614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.706700</td>\n",
       "      <td>0.698655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.702300</td>\n",
       "      <td>0.698185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.706200</td>\n",
       "      <td>0.697405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.689300</td>\n",
       "      <td>0.697134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.707300</td>\n",
       "      <td>0.697241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.685300</td>\n",
       "      <td>0.696786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.694400</td>\n",
       "      <td>0.696670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.716300</td>\n",
       "      <td>0.696777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.703900</td>\n",
       "      <td>0.696849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.706900</td>\n",
       "      <td>0.695982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.690200</td>\n",
       "      <td>0.695572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.708200</td>\n",
       "      <td>0.695772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.697300</td>\n",
       "      <td>0.695712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.696095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.695687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.701100</td>\n",
       "      <td>0.694859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.687200</td>\n",
       "      <td>0.694698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.678800</td>\n",
       "      <td>0.694618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.683500</td>\n",
       "      <td>0.694729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.701100</td>\n",
       "      <td>0.694789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.698400</td>\n",
       "      <td>0.694025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.687800</td>\n",
       "      <td>0.693873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.698900</td>\n",
       "      <td>0.694415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.704400</td>\n",
       "      <td>0.694091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.716300</td>\n",
       "      <td>0.692961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.682600</td>\n",
       "      <td>0.692686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.692500</td>\n",
       "      <td>0.692443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.693100</td>\n",
       "      <td>0.692315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.694300</td>\n",
       "      <td>0.692085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.706900</td>\n",
       "      <td>0.691882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>0.691946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.693400</td>\n",
       "      <td>0.691403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.684100</td>\n",
       "      <td>0.691101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.699600</td>\n",
       "      <td>0.691222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.706300</td>\n",
       "      <td>0.691569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.693400</td>\n",
       "      <td>0.691099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.693300</td>\n",
       "      <td>0.691784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.668700</td>\n",
       "      <td>0.691675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.699600</td>\n",
       "      <td>0.689971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.692700</td>\n",
       "      <td>0.689349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.698900</td>\n",
       "      <td>0.689121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.691500</td>\n",
       "      <td>0.689394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.711400</td>\n",
       "      <td>0.688944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.697700</td>\n",
       "      <td>0.689067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>0.689069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.691500</td>\n",
       "      <td>0.688488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.689900</td>\n",
       "      <td>0.688292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.693500</td>\n",
       "      <td>0.688252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.698900</td>\n",
       "      <td>0.688429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.677400</td>\n",
       "      <td>0.688734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.691100</td>\n",
       "      <td>0.688401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.709500</td>\n",
       "      <td>0.687775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.700700</td>\n",
       "      <td>0.687639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.684500</td>\n",
       "      <td>0.687886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.684800</td>\n",
       "      <td>0.688418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.730100</td>\n",
       "      <td>0.688085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.696900</td>\n",
       "      <td>0.687600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.685100</td>\n",
       "      <td>0.687545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.689600</td>\n",
       "      <td>0.687499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.680600</td>\n",
       "      <td>0.687430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.668100</td>\n",
       "      <td>0.687228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.708100</td>\n",
       "      <td>0.687070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.679200</td>\n",
       "      <td>0.686991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.690300</td>\n",
       "      <td>0.686894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.688400</td>\n",
       "      <td>0.686838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.683300</td>\n",
       "      <td>0.686839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=760, training_loss=0.6956515613355134, metrics={'train_runtime': 8097.5411, 'train_samples_per_second': 0.741, 'train_steps_per_second': 0.094, 'total_flos': 197333291520000.0, 'train_loss': 0.6956515613355134, 'epoch': 20.0})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(texts, truncation=True, padding=True, max_length=64)\n",
    "train_dataset = CustomDataset(train_encodings, labels)\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "# Freeze the BERT model parameters\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",  # This might still throw a warning, but you can try 'eval_strategy' instead\n",
    ")\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f979968-5e28-49d9-a54a-11d1cd9a859d",
   "metadata": {},
   "source": [
    "#### evaluate the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "98114600-4409-44fb-a335-f830788a5137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
      "        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "test_encodings = tokenizer(texts_test, truncation=True, padding=True, max_length=64)\n",
    "test_dataset = CustomDataset(test_encodings, labels_test)  # Use the same data for validation for this example\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Convert predictions to PyTorch tensor\n",
    "predictions_tensor = torch.tensor(predictions.predictions)\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_labels = torch.argmax(predictions_tensor, dim=1)\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1a6f54b4-d2c6-4fe1-b462-d4434b0e9d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22, 28],\n",
       "       [12, 38]], dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(labels_test, predicted_labels.numpy())\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eb579f5b-8cd2-4bc0-b701-e0140c4d2631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.0 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(labels_test, predicted_labels.numpy())\n",
    "print(f'{accuracy*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e1091d-5199-4199-a510-1e1e10465e44",
   "metadata": {},
   "source": [
    "# Conclusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e96a24d9-dbfc-4706-9cd0-358c104ad840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Update Weights</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Number of training sample</th>\n",
       "      <th>Number of token</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Training time (s)</th>\n",
       "      <th>Number of test samples</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All layer</td>\n",
       "      <td>Random</td>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>50</td>\n",
       "      <td>97.66</td>\n",
       "      <td>3</td>\n",
       "      <td>66%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All layer</td>\n",
       "      <td>IMDB</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>147.75</td>\n",
       "      <td>100</td>\n",
       "      <td>47%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All layer</td>\n",
       "      <td>IMDB</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>276.64</td>\n",
       "      <td>100</td>\n",
       "      <td>46%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All layer</td>\n",
       "      <td>IMDB</td>\n",
       "      <td>100</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>445.45</td>\n",
       "      <td>100</td>\n",
       "      <td>51%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All layer</td>\n",
       "      <td>IMDB</td>\n",
       "      <td>300</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>11355.75</td>\n",
       "      <td>300</td>\n",
       "      <td>66%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Last layer</td>\n",
       "      <td>IMDB</td>\n",
       "      <td>300</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>8097.54</td>\n",
       "      <td>300</td>\n",
       "      <td>60%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Update Weights Dataset  Number of training sample  Number of token  Epoch  \\\n",
       "0      All layer  Random                          5              128     50   \n",
       "1      All layer    IMDB                        100                4      5   \n",
       "2      All layer    IMDB                        100               16      5   \n",
       "3      All layer    IMDB                        100               32      5   \n",
       "4      All layer    IMDB                        300               64     20   \n",
       "5     Last layer    IMDB                        300               64     20   \n",
       "\n",
       "   Training time (s)  Number of test samples Accuracy  \n",
       "0              97.66                       3      66%  \n",
       "1             147.75                     100      47%  \n",
       "2             276.64                     100      46%  \n",
       "3             445.45                     100      51%  \n",
       "4           11355.75                     300      66%  \n",
       "5            8097.54                     300      60%  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dictionary with the data\n",
    "data = {\n",
    "    \"Update Weights\": [\"All layer\", \"All layer\", \"All layer\", \"All layer\", \"All layer\", \"Last layer\"],\n",
    "    \"Dataset\": [\"Random\", \"IMDB\", \"IMDB\", \"IMDB\", \"IMDB\", \"IMDB\"],\n",
    "    \"Number of training sample\": [5, 100, 100, 100, 300, 300],\n",
    "    \"Number of token\": [128, 4, 16, 32, 64, 64],\n",
    "    \"Epoch\": [50, 5, 5, 5, 20, 20],\n",
    "    \"Training time (s)\": [97.66, 147.75, 276.64, 445.45, 11355.75, 8097.54],\n",
    "    \"Number of test samples\": [3, 100, 100, 100, 300, 300],\n",
    "    \"Accuracy\": [\"66%\", \"47%\", \"46%\", \"51%\", \"66%\", \"60%\"]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "result = pd.DataFrame(data)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb092c43-7724-4156-80b9-ea877437e7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7b2a5-abda-4e68-8ee7-c17cee372002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2470c82-37e9-4192-8995-43dfa6d0dcaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13340457-4672-4a09-a236-e004ea494d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d87edb-6104-41f8-865f-420052202b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575437d9-ba61-42f0-81b7-4bd2fd27440c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61899ca-2701-47c9-b0bb-2b7410d8713c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7ad53-13e8-4c84-ac7a-7c715a34b391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7f0c65-8cd9-4cfa-b84d-8a61962e7525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eccd5a2-615a-4d65-ac7e-071f7be9772f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c22339f-1a90-4c78-8f10-f615c03246c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
