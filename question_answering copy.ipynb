{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "\n",
    "### The dataset is called SQUAD.\n",
    "\n",
    "### each datapoint is a question, a context that may contain the answer to the question, the start index of the answer, the answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.46.0\n",
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(transformers.__version__) # \"transformers_version\": \"4.15.0\",\n",
    "print(tf.__version__) # TensorFlow version 2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squad_v2 = False\n",
    "# model_checkpoint = \"distilbert-base-uncased\"\n",
    "# batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? => {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]} \n",
      "\n",
      "What is in front of the Notre Dame Main Building? => {'text': ['a copper statue of Christ'], 'answer_start': [188]} \n",
      "\n",
      "The Basilica of the Sacred heart at Notre Dame is beside to which structure? => {'text': ['the Main Building'], 'answer_start': [279]} \n",
      "\n",
      "What is the Grotto at Notre Dame? => {'text': ['a Marian place of prayer and reflection'], 'answer_start': [381]} \n",
      "\n",
      "What sits on top of the Main Building at Notre Dame? => {'text': ['a golden statue of the Virgin Mary'], 'answer_start': [92]} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q, a in zip(dataset[\"train\"][\"question\"][:5], dataset[\"train\"][\"answers\"]):\n",
    "    print(f\"{q} => {a} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this function adds the end of the answer to the dataset, and does some sanity checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_end_index(answers, contexts):\n",
    "    \"\"\"Add end index to answers\"\"\"\n",
    "    # print(contexts)\n",
    "    fixed_answers = []\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer[\"text\"][0]\n",
    "        # print(gold_text)\n",
    "        answer[\"text\"] = gold_text\n",
    "        # print(answer[\"text\"])\n",
    "\n",
    "        start_idx = answer[\"answer_start\"][0]\n",
    "        answer[\"answer_start\"] = start_idx\n",
    "\n",
    "        # Make sure the starting index is valid and there is an answer\n",
    "        assert start_idx >= 0 and len(gold_text.strip()) > 0\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "        answer[\"answer_end\"] = end_idx\n",
    "\n",
    "        # Make sure the corresponding context matches the actual answer\n",
    "        assert context[start_idx:end_idx] == gold_text\n",
    "\n",
    "        fixed_answers.append(answer)\n",
    "    return fixed_answers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n"
     ]
    }
   ],
   "source": [
    "for answer in dataset[\"train\"][\"answers\"]:\n",
    "    print(answer)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data corrections\n",
      "\n",
      "Validation data correction\n"
     ]
    }
   ],
   "source": [
    "train_questions = dataset[\"train\"][\"question\"]\n",
    "print(\"Training data corrections\")\n",
    "train_answers, train_contexts = compute_end_index(\n",
    "    dataset[\"train\"][\"answers\"], dataset[\"train\"][\"context\"]\n",
    ")\n",
    "\n",
    "test_questions = dataset[\"validation\"][\"question\"]\n",
    "print(\"\\nValidation data correction\")\n",
    "test_answers, test_contexts = compute_end_index(\n",
    "    dataset[\"validation\"][\"answers\"], dataset[\"validation\"][\"context\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    \"bert-base-uncased\"\n",
    ")  # using the pretrained tokenizer (there is no distinction between uppercase and lowercase)\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\"\"\"\n",
    "text - A single or batch of text sequences to be encoded by the tokenizer\n",
    "text_pair - An optinal single or batch of text sequences to be encoded by the tokenizer. useful for multipair(question and context)\n",
    "\"\"\"\n",
    "\n",
    "context = \"This is the context\"\n",
    "question = \"This is the question\"\n",
    "token_ids = tokenizer(\n",
    "    text=context, text_pair=question,\n",
    "    padding=False, return_tensors='tf'\n",
    ")\n",
    "\n",
    "\n",
    "# tokenizer.convert_ids_to_tokens(token_ids[\"input_ids\"].numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'this', 'is', 'the', 'context', '[SEP]', 'this', 'is', 'the', 'question', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(token_ids[\"input_ids\"].numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the train and test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_encodings.shape: (87599, 512)\n"
     ]
    }
   ],
   "source": [
    "# Encoding train\n",
    "train_encodings = tokenizer(\n",
    "    train_questions, train_contexts, truncation=True, padding=True, return_tensors=\"tf\"\n",
    ")\n",
    "\n",
    "# Encoding test\n",
    "test_encodings = tokenizer(\n",
    "    test_questions, test_contexts, truncation=True, padding=True, return_tensors=\"tf\"\n",
    ")\n",
    "\n",
    "print(\"train_encodings.shape: {}\".format(train_encodings[\"input_ids\"].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.convert_ids_to_tokens(train_encodings[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the character-based indices to token-based indecis\n",
    "\n",
    "\n",
    "def replace_char_with_token_indices(encodings, answers):\n",
    "    \"\"\"\n",
    "    This function takes a set of BatchEncodings called encodings generated by the tokenizer and a set of\n",
    "    answers( a list of dictionaries). Then it updates the provided encodings with two new keys:\n",
    "    start_position and end_positions. These keys respectively hold the token-based indices denoting the\n",
    "    start and end of the answer. if the asnwer is not found, set the start and end indices to the last token.\n",
    "    To convert the existing character-based indeces to token-based indices, call char_to_token() provided by\n",
    "    the BatchEncodings class.\n",
    "    \"\"\"\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    n_updates = 0\n",
    "\n",
    "    # Go through all the answers\n",
    "    for i in range(len(answers)):\n",
    "        # Get the token position for both start and end char positions\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i][\"answer_start\"]))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i][\"answer_end\"] - 1))\n",
    "\n",
    "        if start_positions[-1] is None or end_positions[-1] is None:\n",
    "            n_updates += 1\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        # https://huggingface.co/transformers/custom_datasets.html#qa-squad\n",
    "\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length - 1\n",
    "\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length - 1\n",
    "\n",
    "    print(\"{}/{} had answers truncated\".format(n_updates, len(answers)))\n",
    "    encodings.update(\n",
    "        {\"start_positions\": start_positions, \"end_positions\": end_positions}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81196/87599 had answers truncated\n",
      "9786/10570 had answers truncated\n"
     ]
    }
   ],
   "source": [
    "replace_char_with_token_indices(train_encodings, train_answers)\n",
    "replace_char_with_token_indices(test_encodings, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/tf2/lib/python3.10/site-packages/transformers/activations_tf.py:22\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/tf2/lib/python3.10/site-packages/transformers/utils/import_utils.py:1778\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2/lib/python3.10/site-packages/transformers/models/bert/modeling_tf_bert.py:28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[1;32m     31\u001b[0m     TFBaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     TFTokenClassifierOutput,\n\u001b[1;32m     39\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2/lib/python3.10/site-packages/transformers/activations_tf.py:27\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m         )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gelu\u001b[39m(x):\n",
      "\u001b[0;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertConfig, TFBertForQuestionAnswering\n\u001b[1;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m BertConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(config)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2/lib/python3.10/site-packages/transformers/utils/import_utils.py:1767\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1766\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m-> 1767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[1;32m   1769\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2/lib/python3.10/site-packages/transformers/utils/import_utils.py:1766\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1764\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1766\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2/lib/python3.10/site-packages/transformers/utils/import_utils.py:1780\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1781\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1783\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, TFBertForQuestionAnswering\n",
    "\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\", return_dict=False)\n",
    "print(config)\n",
    "\n",
    "model = TFBertForQuestionAnswering.from_pretrained(\"bert-base-uncased\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Tensorflow dataset\n",
    "\n",
    "data will\n",
    "consist of two tuples: one containing inputs and the other containing the targets.\n",
    "\n",
    "### The input tuple contains:\n",
    "\n",
    "- Input token IDs – A batch of padded token IDs of size [batch size, sequence length]\n",
    "- Attention mask – A batch of attention masks of size [batch size, sequence length]\n",
    "\n",
    "### The output tuple contains:\n",
    "\n",
    "- Start index of the answer – A batch of start indices of the answer\n",
    "- End index of the answer – A batch of end indices of the answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(input_ids, attension_mask, start_positions, end_positions):\n",
    "    \"\"\"generator for data\n",
    "\n",
    "    Since the data is already processed, it's a matter of reorganizing the already existing data to return\n",
    "    \"\"\"\n",
    "\n",
    "    for inps, attn, start_pos, end_pos in zip(\n",
    "        input_ids, attension_mask, start_positions, end_positions\n",
    "    ):\n",
    "        yield (inps, attn), (start_pos, end_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Done\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the generator as a callable\n",
    "train_data_gen = partial(\n",
    "    data_gen,\n",
    "    input_ids=train_encodings[\"input_ids\"],\n",
    "    attention_mask=train_encodings[\"attention_mask\"],\n",
    "    start_positions=train_encodings[\"start_positions\"],\n",
    "    end_positions=train_encodings[\"end_positions\"],\n",
    ")\n",
    "\n",
    "# Define the dataset\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    train_data_gen, output_types=((\"int32\", \"int32\"), (\"int32\", \"int32\"))\n",
    ")\n",
    "\n",
    "# shuffle the data\n",
    "train_dataset = train_dataset.shuffle(1000)\n",
    "print(\"\\t Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split our dataset into two: a training set and a validation dataset. We will use the first\n",
    "\n",
    "10,000 samples as the validation set. The rest of the data is used as the training set. Both datasets\n",
    "will be batched using a batch size of 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid set is taken as the first 10000 samples in the shuffled set\n",
    "valid_dataset = train_dataset.take(10000)\n",
    "valid_dataset = valid_dataset.batch(4)\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.skip(10000)\n",
    "train_dataset = train_dataset.batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in train_dataset.take(0):\n",
    "    print(item.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test data\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating test data\")\n",
    "\n",
    "# Generator callable\n",
    "test_data_gen = partial(\n",
    "    data_gen,\n",
    "    input_ids=test_encodings[\"input_ids\"],\n",
    "    attention_mask=test_encodings[\"attention_mask\"],\n",
    "    start_positions=test_encodings[\"start_positions\"],\n",
    "    end_positions=test_encodings[\"end_positions\"],\n",
    ")\n",
    "\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    test_data_gen, output_types=((\"int32\", \"int32\"), (\"int32\", \"int32\"))\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"return_dict\": false,\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, TFBertForQuestionAnswering\n",
    "\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\", return_dict=False)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForQuestionAnswering were not initialized from the PyTorch model and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForQuestionAnswering.from_pretrained(\"bert-base-uncased\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_wrap_model(model):\n",
    "    \"\"\"Wraps the huggingface's model with in the Keras Functional API\"\"\"\n",
    "    # Define inputs\n",
    "    input_ids = tf.keras.layers.Input(\n",
    "        [\n",
    "            None,\n",
    "        ],\n",
    "        dtype=tf.int32,\n",
    "        name=\"input_ids\",\n",
    "    )\n",
    "    attention_mask = tf.keras.layers.Input(\n",
    "        [\n",
    "            None,\n",
    "        ],\n",
    "        dtype=tf.int32,\n",
    "        name=\"attention_mask\",\n",
    "    )\n",
    "    # Define the output (TFQuestionAnsweringModelOutput)\n",
    "    out = model(input_ids)\n",
    "\n",
    "    # print(dir(out))\n",
    "    # Get the correct attributes in the produced object to generate an\n",
    "    # output tuple\n",
    "    wrap_model = tf.keras.models.Model(\n",
    "        [input_ids, attention_mask], outputs=(out.start_logits, out.end_logits)\n",
    "    )\n",
    "    return wrap_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'start_logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[165], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m acc \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mSparseCategoricalAccuracy()\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m model_v2 \u001b[38;5;241m=\u001b[39m \u001b[43mtf_wrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model_v2\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39mloss, metrics\u001b[38;5;241m=\u001b[39m[acc])\n",
      "Cell \u001b[0;32mIn[164], line 25\u001b[0m, in \u001b[0;36mtf_wrap_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     19\u001b[0m out \u001b[38;5;241m=\u001b[39m model(input_ids)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(dir(out))\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Get the correct attributes in the produced object to generate an\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# output tuple\u001b[39;00m\n\u001b[1;32m     24\u001b[0m wrap_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mModel(\n\u001b[0;32m---> 25\u001b[0m     [input_ids, attention_mask], outputs\u001b[38;5;241m=\u001b[39m(\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_logits\u001b[49m, out\u001b[38;5;241m.\u001b[39mend_logits)\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_model\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'start_logits'"
     ]
    }
   ],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "model_v2 = tf_wrap_model(model)\n",
    "model_v2.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TFBertForQuestionAnswering.from_pretrained(\"ydshieh/bert-base-cased-squad2\")\n",
    "\n",
    "\n",
    "# question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "\n",
    "# inputs = tokenizer(question, text, return_tensors=\"tf\")\n",
    "# outputs = model(**inputs)\n",
    "\n",
    "# answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n",
    "# answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n",
    "\n",
    "# predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "# tokenizer.decode(predict_answer_tokens)\n",
    "# # \"a nice puppet\"\n",
    "\n",
    "# print(outputs.start_logits)\n",
    "# print(dir(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'start_logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m acc \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mSparseCategoricalAccuracy()\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m model_v2 \u001b[38;5;241m=\u001b[39m \u001b[43mtf_wrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model_v2\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39mloss, metrics\u001b[38;5;241m=\u001b[39m[acc])\n",
      "Cell \u001b[0;32mIn[90], line 25\u001b[0m, in \u001b[0;36mtf_wrap_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     19\u001b[0m out \u001b[38;5;241m=\u001b[39m model([input_ids, attention_mask])\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(dir(out))\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Get the correct attributes in the produced object to generate an\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# output tuple\u001b[39;00m\n\u001b[1;32m     24\u001b[0m wrap_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mModel(\n\u001b[0;32m---> 25\u001b[0m     [input_ids, attention_mask], outputs\u001b[38;5;241m=\u001b[39m(\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_logits\u001b[49m, out\u001b[38;5;241m.\u001b[39mend_logits)\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_model\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'start_logits'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v2.fit(train_dataset, validation_data=valid_dataset, epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
